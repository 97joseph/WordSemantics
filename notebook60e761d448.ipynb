{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **CS 6120: Natural Language Processing - Prof. Ahmad Uzair** \n\n### **Assignment 3: Parts Of Speech Tagging**\n\n### **Total points: 120**\n\nIn this assignment, you will have hands-on experience with HMMs - Viterbi algorithm and RNNs with PyTorch. You'll will be doing part-of-speech (POS) tagging, the process of assigning a part-of-speech tag (Noun, Verb, Adjective...) to each word in an input text.\n\n\nFew tips:\n- We expect this assignment to take longer than assignment 1 and assignment 2, so start early.<br>\n- We strongly recommend going through the relevant course modules and all the resources linked to gain strong conceptual understanding before you start.\n- Course Modules, SLP Text book and PyTorch Documentation are the best resources. \n- Completing Part 2 requires Part 1 but Part 3 does not require any dependencies from the previous parts. \n\nBefore you start, If you need a refresher for part 1,2, \n- https://web.stanford.edu/~jurafsky/slp3/8.pdf","metadata":{"id":"236da79b"}},{"cell_type":"code","source":"# Importing packages and loading in the data set \nimport pandas as pd\nfrom collections import defaultdict\nimport math\nimport numpy as np","metadata":{"id":"b1b59ba3","execution":{"iopub.status.busy":"2022-03-26T06:56:57.559848Z","iopub.execute_input":"2022-03-26T06:56:57.560138Z","iopub.status.idle":"2022-03-26T06:56:57.565509Z","shell.execute_reply.started":"2022-03-26T06:56:57.560108Z","shell.execute_reply":"2022-03-26T06:56:57.564469Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import string\n\n# Punctuation characters\npunct = set(string.punctuation)\n\n# Morphology rules used to assign unknown word tokens\nnoun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\nverb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\nadj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\nadv_suffix = [\"ward\", \"wards\", \"wise\"]\n\n\ndef get_word_tag(line, vocab): \n    if not line.split():\n        word = \"--n--\"\n        tag = \"--s--\"\n        return word, tag\n    else:\n        word, tag = line.split()\n        if word not in vocab: \n            # Handle unknown words\n            word = assign_unk(word)\n        return word, tag\n    return None \n\n\ndef preprocess(vocab, data_fp):\n    \"\"\"\n    Preprocess data\n    \"\"\"\n    orig = []\n    prep = []\n\n    # Read data\n    with open(data_fp, \"r\") as data_file:\n\n        for cnt, word in enumerate(data_file):\n\n            # End of sentence\n            if not word.split():\n                orig.append(word.strip())\n                word = \"--n--\"\n                prep.append(word)\n                continue\n\n            # Handle unknown words\n            elif word.strip() not in vocab:\n                orig.append(word.strip())\n                word = assign_unk(word)\n                prep.append(word)\n                continue\n\n            else:\n                orig.append(word.strip())\n                prep.append(word.strip())\n\n    assert(len(orig) == len(open(data_fp, \"r\").readlines()))\n    assert(len(prep) == len(open(data_fp, \"r\").readlines()))\n\n    return orig, prep\n\n\ndef assign_unk(tok):\n    \"\"\"\n    Assign unknown word tokens\n    \"\"\"\n    # Digits\n    if any(char.isdigit() for char in tok):\n        return \"--unk_digit--\"\n\n    # Punctuation\n    elif any(char in punct for char in tok):\n        return \"--unk_punct--\"\n\n    # Upper-case\n    elif any(char.isupper() for char in tok):\n        return \"--unk_upper--\"\n\n    # Nouns\n    elif any(tok.endswith(suffix) for suffix in noun_suffix):\n        return \"--unk_noun--\"\n\n    # Verbs\n    elif any(tok.endswith(suffix) for suffix in verb_suffix):\n        return \"--unk_verb--\"\n\n    # Adjectives\n    elif any(tok.endswith(suffix) for suffix in adj_suffix):\n        return \"--unk_adj--\"\n\n    # Adverbs\n    elif any(tok.endswith(suffix) for suffix in adv_suffix):\n        return \"--unk_adv--\"\n\n    return \"--unk--\"\n","metadata":{"id":"388e90af","execution":{"iopub.status.busy":"2022-03-26T06:56:57.572922Z","iopub.execute_input":"2022-03-26T06:56:57.573863Z","iopub.status.idle":"2022-03-26T06:56:57.592981Z","shell.execute_reply.started":"2022-03-26T06:56:57.573777Z","shell.execute_reply":"2022-03-26T06:56:57.591963Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"<a name='0'></a>\n## Data Sources\nThis assignment will use two tagged data sets collected from the **Wall Street Journal (WSJ)**. \n\n[Here](http://relearn.be/2015/training-common-sense/sources/software/pattern-2.6-critical-fork/docs/html/mbsp-tags.html) is an example 'tag-set' or Part of Speech designation describing the two or three letter tag and their meaning. \n- One data set (**WSJ-2_21.pos**) will be used for **training**.\n- The other (**WSJ-24.pos**) for **testing**. \n- The tagged training data has been preprocessed to form a vocabulary (**hmm_vocab.txt**). \n- The words in the vocabulary are words from the training set that were used two or more times. \n- The vocabulary is augmented with a set of 'unknown word tokens', described below. \n\nThe training set will be used to create the emission, transmission and tag counts. \n\nThe test set (WSJ-24.pos) is read in to create `y`. \n- This contains both the test text and the true tag. \n- The test set has also been preprocessed to remove the tags to form **test.words.txt**. \n- This is read in and further processed to identify the end of sentences and handle words not in the vocabulary using functions provided above. \n- This forms the list `prep`, the preprocessed text used to test our  POS taggers.\n\nA POS tagger will necessarily encounter words that are not in its datasets. \n- To improve accuracy, these words are further analyzed during preprocessing to extract available hints as to their appropriate tag. \n- For example, the suffix 'ize' is a hint that the word is a verb, as in 'final-ize' or 'character-ize'. \n- A set of unknown-tokens, such as '--unk-verb--' or '--unk-noun--' will replace the unknown words in both the training and test corpus and will appear in the emission, transmission and tag data structures.\n\n<img src = \"./DataSources1.PNG\" />","metadata":{"id":"81ac2f35"}},{"cell_type":"markdown","source":"Implementation note: \n\n- For python 3.6 and beyond, dictionaries retain the insertion order. \n- Furthermore, their hash-based lookup makes them suitable for rapid membership tests. \n    - If _di_ is a dictionary, `key in di` will return `True` if _di_ has a key _key_, else `False`. \n\nThe dictionary `vocab` will utilize these features.","metadata":{"id":"4e04a88f"}},{"cell_type":"markdown","source":"<a name='1'></a>\n# Part 1: Parts-of-speech tagging - Baseline\n\n<a name='1.1'></a>\n## Part 1.1 - Training\nYou will start with the simplest possible parts-of-speech tagger and we will build up to the state of the art. \n\nIn this section, you will find the words that are not ambiguous. \n- For example, the word `is` is a verb and it is not ambiguous. \n- In the `WSJ` corpus, $86$% of the token are unambiguous (meaning they have only one tag) \n- About $14\\%$ are ambiguous (meaning that they have more than one tag)\n\n<img src = \"pos.png\" style=\"width:400px;height:250px;\"/>\n\nBefore you start predicting the tags of each word, you will need to compute a few dictionaries that will help you to generate the tables. \n","metadata":{"id":"60626702"}},{"cell_type":"markdown","source":"#### Transition counts\n- The first dictionary is the `transition_counts` dictionary which computes the number of times each tag happened next to another tag. \n\nThis dictionary will be used to compute: \n$$P(t_i |t_{i-1}) \\tag{1}$$\n\nThis is the probability of a tag at position $i$ given the tag at position $i-1$.\n\nIn order for you to compute equation 1, you will create a `transition_counts` dictionary where \n- The keys are `(prev_tag, tag)`\n- The values are the number of times those two tags appeared in that order. ","metadata":{"id":"8e51520e"}},{"cell_type":"markdown","source":"#### Emission counts\n\nThe second dictionary you will compute is the `emission_counts` dictionary. This dictionary will be used to compute:\n\n$$P(w_i|t_i)\\tag{2}$$\n\nIn other words, you will use it to compute the probability of a word given its tag. \n\nIn order for you to compute equation 2, you will create an `emission_counts` dictionary where \n- The keys are `(tag, word)` \n- The values are the number of times that pair showed up in your training set. ","metadata":{"id":"00f0a66a"}},{"cell_type":"markdown","source":"#### Tag counts\n\nThe last dictionary you will compute is the `tag_counts` dictionary. \n- The key is the tag \n- The value is the number of times each tag appeared.","metadata":{"id":"c4cb3b73"}},{"cell_type":"markdown","source":"<a name='ex-01'></a>\n### Exercise 01 - 5 Points\n\n**Instructions:** Write a program that takes in the `training_corpus` and returns the three dictionaries mentioned above `transition_counts`, `emission_counts`, and `tag_counts`. \n- `emission_counts`: maps (tag, word) to the number of times it happened. (Required for baseline and HMM)\n- `transition_counts`: maps (prev_tag, tag) to the number of times it has appeared. (for HMM)\n- `tag_counts`: maps (tag) to the number of times it has occured. (for HMM)\n\nImplementation note: This routine utilises *defaultdict*, which is a subclass of *dict*. \n- A standard Python dictionary throws a *KeyError* if you try to access an item with a key that is not currently in the dictionary. \n- In contrast, the *defaultdict* will create an item of the type of the argument, in this case an integer with the default value of 0. \n- See [defaultdict](https://docs.python.org/3.3/library/collections.html#defaultdict-objects).","metadata":{"id":"dd34f40c"}},{"cell_type":"code","source":"# Do not change anything in this cell\n# load in the training corpus\nwith open(\"WSJ-2_21.pos\", 'r') as f:\n    training_corpus = f.readlines()\n\n# read the vocabulary data, split by each line of text, and save the list\nwith open(\"hmm_vocab.txt\", 'r') as f:\n    voc_l = f.read().split('\\n')\n\n# vocab: dictionary that has the index of the corresponding words\nvocab = {} \n\n# Get the index of the corresponding words. \nfor i, word in enumerate(sorted(voc_l)): \n    vocab[word] = i       \n    \n\ncnt = 0\nfor k,v in vocab.items():\n    cnt += 1\n    if cnt > 20:\n        break\n\n# load in the test corpus\nwith open(\"WSJ-24.pos\", 'r') as f:\n    y = f.readlines()\n\n#corpus without tags, preprocessed\n_, prep = preprocess(vocab, \"test.words.txt\")","metadata":{"id":"a4e84bdf","execution":{"iopub.status.busy":"2022-03-26T06:56:57.594726Z","iopub.execute_input":"2022-03-26T06:56:57.595504Z","iopub.status.idle":"2022-03-26T06:56:57.620219Z","shell.execute_reply.started":"2022-03-26T06:56:57.595462Z","shell.execute_reply":"2022-03-26T06:56:57.618974Z"},"trusted":true},"execution_count":26,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_34/657402892.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Do not change anything in this cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# load in the training corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WSJ-2_21.pos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtraining_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'WSJ-2_21.pos'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'WSJ-2_21.pos'","output_type":"error"}]},{"cell_type":"code","source":"# TASK CELL\n\ndef create_dictionaries(training_corpus, vocab):\n    \"\"\"\n    Input: \n        training_corpus: a corpus where each line has a word followed by its tag.\n        vocab: a dictionary where keys are words in vocabulary and value is an index\n    Output: \n        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts\n        tag_counts: a dictionary where the keys are the tags and the values are the counts\n    \"\"\"\n    \n    # initialize the dictionaries using defaultdict\n    emission_counts = defaultdict(int)\n    transition_counts = defaultdict(int)\n    tag_counts = defaultdict(int)\n    \n    # Initialize \"prev_tag\" (previous tag) with the start state, denoted by '--s--'\n    prev_tag = '--s--' \n    \n    # use 'i' to track the line number in the corpus\n    i = 0 \n    \n    # Each item in the training corpus contains a word and its POS tag\n    # Go through each word and its tag in the training corpus\n    for word_tag in training_corpus:\n        \n        # Increment the word_tag count\n        i += 1\n        \n        # Every 50,000 words, print the word count\n        if i % 50000 == 0:\n            print(f\"word count = {i}\")\n            \n        ### START CODE HERE (Replace instances of 'None' with your code) ###\n        # get the word and tag using the get_word_tag helper function\n        \n        # Increment the transition count for the previous word and tag\n        \n        # Increment the emission count for the tag and word\n\n        # Increment the tag count\n\n        # Set the previous tag to this tag (for the next iteration of the loop)\n        \n        ### END CODE HERE ###\n        \n    return emission_counts, transition_counts, tag_counts","metadata":{"id":"278a7d07","execution":{"iopub.status.busy":"2022-03-26T06:56:57.621532Z","iopub.status.idle":"2022-03-26T06:56:57.622578Z","shell.execute_reply.started":"2022-03-26T06:56:57.622144Z","shell.execute_reply":"2022-03-26T06:56:57.622171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)","metadata":{"id":"960dd1e8","execution":{"iopub.status.busy":"2022-03-26T06:56:57.623865Z","iopub.status.idle":"2022-03-26T06:56:57.624295Z","shell.execute_reply.started":"2022-03-26T06:56:57.624129Z","shell.execute_reply":"2022-03-26T06:56:57.624148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get all the POS states\nstates = sorted(tag_counts.keys())\nprint(f\"Number of POS tags (number of 'states'): {len(states)}\")\nprint(\"View these POS tags (states)\")\nprint(states)","metadata":{"id":"b386a15a","execution":{"iopub.status.busy":"2022-03-26T06:56:57.625460Z","iopub.status.idle":"2022-03-26T06:56:57.625956Z","shell.execute_reply.started":"2022-03-26T06:56:57.625711Z","shell.execute_reply":"2022-03-26T06:56:57.625729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Expected Output\n\n```CPP\nNumber of POS tags (number of 'states'46\nView these states\n['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n```","metadata":{"id":"688c5c9e"}},{"cell_type":"markdown","source":"The 'states' are the Parts-of-speech designations found in the training data. They will also be referred to as 'tags' or POS in this assignment. \n\n- \"NN\" is noun, singular, \n- 'NNS' is noun, plural. \n- In addition, there are helpful tags like '--s--' which indicate a start of a sentence.\n- You can get a more complete description at [Penn Treebank II tag set](https://www.clips.uantwerpen.be/pages/mbsp-tags). ","metadata":{"id":"e5030945"}},{"cell_type":"code","source":"print(\"transition examples: \")\nfor ex in list(transition_counts.items())[:3]:\n    print(ex)\nprint()\n\nprint(\"emission examples: \")\nfor ex in list(emission_counts.items())[200:203]:\n    print (ex)\nprint()\n\nprint(\"ambiguous word example: \")\nfor tup,cnt in emission_counts.items():\n    if tup[1] == 'back': print (tup, cnt) ","metadata":{"id":"d80b55e8","execution":{"iopub.status.busy":"2022-03-26T06:56:57.627015Z","iopub.status.idle":"2022-03-26T06:56:57.627537Z","shell.execute_reply.started":"2022-03-26T06:56:57.627342Z","shell.execute_reply":"2022-03-26T06:56:57.627361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Expected Output\n\n```CPP\ntransition examples: \n(('--s--', 'IN'), 5050)\n(('IN', 'DT'), 32364)\n(('DT', 'NNP'), 9044)\n\nemission examples: \n(('DT', 'any'), 721)\n(('NN', 'decrease'), 7)\n(('NN', 'insider-trading'), 5)\n\nambiguous word example: \n('RB', 'back') 304\n('VB', 'back') 20\n('RP', 'back') 84\n('JJ', 'back') 25\n('NN', 'back') 29\n('VBP', 'back') 4\n```","metadata":{"id":"274104ea"}},{"cell_type":"markdown","source":"<a name='1.2'></a>\n### Part 1.2 - Testing\n\nNow you will test the accuracy of your parts-of-speech tagger using your `emission_counts` dictionary. \n- Given your preprocessed test corpus `prep`, you will assign a parts-of-speech tag to every word in that corpus. \n- Using the original tagged test corpus `y`, you will then compute what percent of the tags you got correct. ","metadata":{"id":"bb67232c"}},{"cell_type":"markdown","source":"<a name='ex-02'></a>\n### Exercise 02 - 15 Points\n\n**Instructions:** Implement `predict_pos` that computes the accuracy of your model. \n\n- This is a Baseline model. \n- To assign a part of speech to a word, assign the most frequent POS for that word in the training set. \n- Then evaluate how well this approach works.  Each time you predict based on the most frequent POS for the given word, check whether the actual POS of that word is the same.  If so, the prediction was correct!\n- Calculate the accuracy as the number of correct predictions divided by the total number of words for which you predicted the POS tag.","metadata":{"id":"c10b275c"}},{"cell_type":"code","source":"# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: predict_pos\n\ndef predict_pos(prep, y, emission_counts, vocab, states):\n    '''\n    Input: \n        prep: a preprocessed version of 'y'. A list with the 'word' component of the tuples.\n        y: a corpus composed of a list of tuples where each tuple consists of (word, POS)\n        emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count\n        vocab: a dictionary where keys are words in vocabulary and value is an index\n        states: a sorted list of all possible tags for this assignment\n    Output: \n        accuracy: Number of times you classified a word correctly\n    '''\n    \n    # Initialize the number of correct predictions to zero\n    \n    # Get the (tag, word) tuples, stored as a set\n    \n    # Get the number of (word, POS) tuples in the corpus 'y'\n\n\n        # Split the (word, POS) string into a list of two items\n\n        \n        # Verify that y_tup contain both word and POS\n\n            \n            # Set the true POS label for this word\n\n\n            # If the y_tup didn't contain word and POS, go to next word\n\n\n        \n        # If the word is in the vocabulary...\n\n\n            ### START CODE HERE (Replace instances of 'None' with your code) ###\n                        \n                # define the key as the tuple containing the POS and word\n\n\n                # check if the (pos, word) key exists in the emission_counts dictionary\n\n                # get the emission count of the (pos,word) tuple \n\n                    # keep track of the POS with the largest count\n\n                        # update the final count (largest count)\n\n                        # update the final POS\n\n            # If the final POS (with the largest count) matches the true POS:\n                \n                # Update the number of correct predictions\n\n            \n    ### END CODE HERE ###\n    accuracy = None\n    \n    return accuracy","metadata":{"id":"b98226d8","execution":{"iopub.status.busy":"2022-03-26T06:56:57.690339Z","iopub.execute_input":"2022-03-26T06:56:57.690898Z","iopub.status.idle":"2022-03-26T06:56:57.697131Z","shell.execute_reply.started":"2022-03-26T06:56:57.690843Z","shell.execute_reply":"2022-03-26T06:56:57.696258Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)\nprint(f\"Accuracy of prediction using predict_pos is {accuracy_predict_pos:.4f}\")","metadata":{"id":"804ba294","execution":{"iopub.status.busy":"2022-03-26T06:56:57.700073Z","iopub.execute_input":"2022-03-26T06:56:57.700346Z","iopub.status.idle":"2022-03-26T06:56:57.724814Z","shell.execute_reply.started":"2022-03-26T06:56:57.700314Z","shell.execute_reply":"2022-03-26T06:56:57.723048Z"},"trusted":true},"execution_count":28,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_34/96285516.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy_predict_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memission_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Accuracy of prediction using predict_pos is {accuracy_predict_pos:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'prep' is not defined"],"ename":"NameError","evalue":"name 'prep' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"##### Expected Output\n```CPP\nAccuracy of prediction using predict_pos is 0.8889\n```\n\n88.9% is really good for a baseline model. With hidden markov models, you should be able to get **95% accuracy.**","metadata":{"id":"99b82a02"}},{"cell_type":"markdown","source":"<a name='2'></a>\n# Part 2: Hidden Markov Models for POS\n\nNow you will build something more context specific. Concretely, you will be implementing a Hidden Markov Model (HMM) with a Viterbi decoder\n- The HMM is one of the most commonly used algorithms in Natural Language Processing, and is a foundation to many deep learning techniques you will see in this specialization. \n- In addition to parts-of-speech tagging, HMM is used in speech recognition, speech synthesis, etc. \n- By completing this part of the assignment you will get a 95% accuracy on the same dataset you used in Part 1.\n\nThe Markov Model contains a number of states and the probability of transition between those states. \n- In this case, the states are the parts-of-speech. \n- A Markov Model utilizes a transition matrix, `A`. \n- A Hidden Markov Model adds an observation or emission matrix `B` which describes the probability of a visible observation when we are in a particular state. \n- In this case, the emissions are the words in the corpus\n- The state, which is hidden, is the POS tag of that word.","metadata":{"id":"204f0836"}},{"cell_type":"markdown","source":"<a name='2.1'></a>\n## Part 2.1 Generating Matrices\n\n### Creating the 'A' transition probabilities matrix\nNow that you have your `emission_counts`, `transition_counts`, and `tag_counts`, you will start implementing the Hidden Markov Model. \n\nThis will allow you to quickly construct the \n- `A` transition probabilities matrix.\n- and the `B` emission probabilities matrix. \n\nYou will also use some smoothing when computing these matrices. \n\nHere is an example of what the `A` transition matrix would look like (it is simplified to 5 tags for viewing. It is 46x46 in this assignment.):\n\n\n|**A**  |...|         RBS  |          RP  |         SYM  |      TO  |          UH|...\n| --- ||---:-------------| ------------ | ------------ | -------- | ---------- |----\n|**RBS**  |...|2.217069e-06  |2.217069e-06  |2.217069e-06  |0.008870  |2.217069e-06|...\n|**RP**   |...|3.756509e-07  |7.516775e-04  |3.756509e-07  |0.051089  |3.756509e-07|...\n|**SYM**  |...|1.722772e-05  |1.722772e-05  |1.722772e-05  |0.000017  |1.722772e-05|...\n|**TO**   |...|4.477336e-05  |4.472863e-08  |4.472863e-08  |0.000090  |4.477336e-05|...\n|**UH**  |...|1.030439e-05  |1.030439e-05  |1.030439e-05  |0.061837  |3.092348e-02|...\n| ... |...| ...          | ...          | ...          | ...      | ...        | ...\n\nNote that the matrix above was computed with smoothing. \n\nEach cell gives you the probability to go from one parts of speech to another. \n- In other words, there is a 4.47e-8 chance of going from parts-of-speech `TO` to `RP`. \n- The sum of each row has to equal 1, because we assume that the next POS tag must be one of the available columns in the table.\n\nThe smoothing was done as follows: \n\n$$ P(t_i | t_{i-1}) = \\frac{C(t_{i-1}, t_{i}) + \\alpha }{C(t_{i-1}) +\\alpha * N}\\tag{3}$$\n\n- $N$ is the total number of tags\n- $C(t_{i-1}, t_{i})$ is the count of the tuple (previous POS, current POS) in `transition_counts` dictionary.\n- $C(t_{i-1})$ is the count of the previous POS in the `tag_counts` dictionary.\n- $\\alpha$ is a smoothing parameter.","metadata":{"id":"5aa91465"}},{"cell_type":"markdown","source":"<a name='ex-03'></a>\n### Exercise 03 - 15 Points\n\n**Instructions:** Implement the `create_transition_matrix` below for all tags. Your task is to output a matrix that computes equation 3 for each cell in matrix `A`. ","metadata":{"id":"e2eec203"}},{"cell_type":"code","source":"# GRADED FUNCTION: create_transition_matrix\ndef create_transition_matrix(alpha, tag_counts, transition_counts):\n    ''' \n    Input: \n        alpha: number used for smoothing\n        tag_counts: a dictionary mapping each tag to its respective count\n        transition_counts: transition count for the previous word and tag\n    Output:\n        A: matrix of dimension (num_tags,num_tags)\n    '''\n    # Write your code here\n    \n    return A","metadata":{"id":"e12d0675","execution":{"iopub.status.busy":"2022-03-26T06:56:57.725915Z","iopub.status.idle":"2022-03-26T06:56:57.726456Z","shell.execute_reply.started":"2022-03-26T06:56:57.726216Z","shell.execute_reply":"2022-03-26T06:56:57.726250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpha = 0.001\nA = create_transition_matrix(alpha, tag_counts, transition_counts)\n# Testing your function\nprint(f\"A at row 0, col 0: {A[0,0]:.9f}\")\nprint(f\"A at row 3, col 1: {A[3,1]:.4f}\")\n\nprint(\"View a subset of transition matrix A\")\nA_sub = pd.DataFrame(A[30:35,30:35], index=states[30:35], columns = states[30:35] )\nprint(A_sub)","metadata":{"id":"7603a95c","execution":{"iopub.status.busy":"2022-03-26T06:56:57.727555Z","iopub.status.idle":"2022-03-26T06:56:57.728149Z","shell.execute_reply.started":"2022-03-26T06:56:57.727863Z","shell.execute_reply":"2022-03-26T06:56:57.727901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Expected Output\n```CPP\nA at row 0, col 0: 0.000007040\nA at row 3, col 1: 0.1691\nView a subset of transition matrix A\n              RBS            RP           SYM        TO            UH\nRBS  2.217069e-06  2.217069e-06  2.217069e-06  0.008870  2.217069e-06\nRP   3.756509e-07  7.516775e-04  3.756509e-07  0.051089  3.756509e-07\nSYM  1.722772e-05  1.722772e-05  1.722772e-05  0.000017  1.722772e-05\nTO   4.477336e-05  4.472863e-08  4.472863e-08  0.000090  4.477336e-05\nUH   1.030439e-05  1.030439e-05  1.030439e-05  0.061837  3.092348e-02\n```","metadata":{"id":"976bd495"}},{"cell_type":"markdown","source":"### Create the 'B' emission probabilities matrix\n\nNow you will create the `B` transition matrix which computes the emission probability. \n\nYou will use smoothing as defined below: \n\n$$P(w_i | t_i) = \\frac{C(t_i, word_i)+ \\alpha}{C(t_{i}) +\\alpha * N}\\tag{4}$$\n\n- $C(t_i, word_i)$ is the number of times $word_i$ was associated with $tag_i$ in the training data (stored in `emission_counts` dictionary).\n- $C(t_i)$ is the number of times $tag_i$ was in the training data (stored in `tag_counts` dictionary).\n- $N$ is the number of words in the vocabulary\n- $\\alpha$ is a smoothing parameter. \n\nThe matrix `B` is of dimension (num_tags, N), where num_tags is the number of possible parts-of-speech tags. \n\nHere is an example of the matrix, only a subset of tags and words are shown: \n<p style='text-align: center;'> <b>B Emissions Probability Matrix (subset)</b>  </p>\n\n|**B**| ...|          725 |     adroitly |    engineers |     promoted |      synergy| ...|\n|----|----|--------------|--------------|--------------|--------------|-------------|----|\n|**CD**  | ...| **8.201296e-05** | 2.732854e-08 | 2.732854e-08 | 2.732854e-08 | 2.732854e-08| ...|\n|**NN**  | ...| 7.521128e-09 | 7.521128e-09 | 7.521128e-09 | 7.521128e-09 | **2.257091e-05**| ...|\n|**NNS** | ...| 1.670013e-08 | 1.670013e-08 |**4.676203e-04** | 1.670013e-08 | 1.670013e-08| ...|\n|**VB**  | ...| 3.779036e-08 | 3.779036e-08 | 3.779036e-08 | 3.779036e-08 | 3.779036e-08| ...|\n|**RB**  | ...| 3.226454e-08 | **6.456135e-05** | 3.226454e-08 | 3.226454e-08 | 3.226454e-08| ...|\n|**RP**  | ...| 3.723317e-07 | 3.723317e-07 | 3.723317e-07 | **3.723317e-07** | 3.723317e-07| ...|\n| ...    | ...|     ...      |     ...      |     ...      |     ...      |     ...      | ...|\n\n","metadata":{"id":"507dc9ee"}},{"cell_type":"markdown","source":"<a name='ex-04'></a>\n### Exercise 04 - 15 Points\n**Instructions:** Implement the `create_emission_matrix` below that computes the `B` emission probabilities matrix. Your function takes in $\\alpha$, the smoothing parameter, `tag_counts`, which is a dictionary mapping each tag to its respective count, the `emission_counts` dictionary where the keys are (tag, word) and the values are the counts. Your task is to output a matrix that computes equation 4 for each cell in matrix `B`. ","metadata":{"id":"8ffb79ce"}},{"cell_type":"code","source":"# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: create_emission_matrix\n\ndef create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n    '''\n    Input: \n        alpha: tuning parameter used in smoothing \n        tag_counts: a dictionary mapping each tag to its respective count\n        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n        vocab: a dictionary where keys are words in vocabulary and value is an index\n    Output:\n        B: a matrix of dimension (num_tags, len(vocab))\n    '''\n    # Write your code here\n    return B","metadata":{"id":"4aef436e","execution":{"iopub.status.busy":"2022-03-26T06:56:57.729200Z","iopub.status.idle":"2022-03-26T06:56:57.729670Z","shell.execute_reply.started":"2022-03-26T06:56:57.729471Z","shell.execute_reply":"2022-03-26T06:56:57.729498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating your emission probability matrix. this takes a few minutes to run. \nB = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n\nprint(f\"View Matrix position at row 0, column 0: {B[0,0]:.9f}\")\nprint(f\"View Matrix position at row 3, column 1: {B[3,1]:.9f}\")\n\n# Try viewing emissions for a few words in a sample dataframe\ncidx  = ['725','adroitly','engineers', 'promoted', 'synergy']\n\n# Get the integer ID for each word\ncols = [vocab[a] for a in cidx]\n\n# Choose POS tags to show in a sample dataframe\nrvals =['CD','NN','NNS', 'VB','RB','RP']\n\n# For each POS tag, get the row number from the 'states' list\nrows = [states.index(a) for a in rvals]\n\n# Get the emissions for the sample of words, and the sample of POS tags\nB_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )\nprint(B_sub)","metadata":{"id":"365490ca","execution":{"iopub.status.busy":"2022-03-26T06:56:57.730710Z","iopub.status.idle":"2022-03-26T06:56:57.731059Z","shell.execute_reply.started":"2022-03-26T06:56:57.730891Z","shell.execute_reply":"2022-03-26T06:56:57.730909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Expected Output\n\n```CPP\nView Matrix position at row 0, column 0: 0.000006032\nView Matrix position at row 3, column 1: 0.000000720\n              725      adroitly     engineers      promoted       synergy\nCD   8.201296e-05  2.732854e-08  2.732854e-08  2.732854e-08  2.732854e-08\nNN   7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05\nNNS  1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08\nVB   3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08\nRB   3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08\nRP   3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07\n```","metadata":{"id":"bd2dce0b"}},{"cell_type":"markdown","source":"<a name='2.2'></a>\n## Part 2.2: Viterbi Algorithm and Dynamic Programming (Extra Credit) - 20 Points\n\nIn this part of the assignment you will implement the Viterbi algorithm which makes use of dynamic programming. Specifically, you will use your two matrices, `A` and `B` to compute the Viterbi algorithm. We have decomposed this process into three main steps for you. \n\n**Note** : You'll get 20 extra points for the part 2.2.\n\n* **Initialization** - In this part you initialize the `best_paths` and `best_probabilities` matrices that you will be populating in `feed_forward`.\n* **Feed forward** - At each step, you calculate the probability of each path happening and the best paths up to that point. \n* **Feed backward**: This allows you to find the best path with the highest probabilities. \n\n<a name='3.1'></a>\n### Part 2.2.1:  Initialization \n\nYou will start by initializing two matrices of the same dimension. \n\n- best_probs: Each cell contains the probability of going from one POS tag to a word in the corpus.\n\n- best_paths: A matrix that helps you trace through the best possible path in the corpus. ","metadata":{"id":"0620ff68"}},{"cell_type":"markdown","source":"<a name='ex-05'></a>\n### Exercise 05 - 5 points\n**Instructions**: \nWrite a program below that initializes the `best_probs` and the `best_paths` matrix. \n\nBoth matrices will be initialized to zero except for column zero of `best_probs`.  \n- Column zero of `best_probs` is initialized with the assumption that the first word of the corpus was preceded by a start token (\"--s--\"). \n- This allows you to reference the **A** matrix for the transition probability\n\nHere is how to initialize column 0 of `best_probs`:\n- The probability of the best path going from the start index to a given POS tag indexed by integer $i$ is denoted by $\\textrm{best_probs}[s_{idx}, i]$.\n- This is estimated as the probability that the start tag transitions to the POS denoted by index $i$: $\\mathbf{A}[s_{idx}, i]$ AND that the POS tag denoted by $i$ emits the first word of the given corpus, which is $\\mathbf{B}[i, vocab[corpus[0]]]$.\n- Note that vocab[corpus[0]] refers to the first word of the corpus (the word at position 0 of the corpus). \n- **vocab** is a dictionary that returns the unique integer that refers to that particular word.\n\nConceptually, it looks like this:\n$\\textrm{best_probs}[s_{idx}, i] = \\mathbf{A}[s_{idx}, i] \\times \\mathbf{B}[i, corpus[0] ]$\n\n\nIn order to avoid multiplying and storing small values on the computer, we'll take the log of the product, which becomes the sum of two logs:\n\n$best\\_probs[i,0] = log(A[s_{idx}, i]) + log(B[i, vocab[corpus[0]]$\n\nAlso, to avoid taking the log of 0 (which is defined as negative infinity), the code itself will just set $best\\_probs[i,0] = float('-inf')$ when $A[s_{idx}, i] == 0$\n\n\nSo the implementation to initialize $best\\_probs$ looks like this:\n\n$ if A[s_{idx}, i] <> 0 : best\\_probs[i,0] = log(A[s_{idx}, i]) + log(B[i, vocab[corpus[0]]])$\n\n$ if A[s_{idx}, i] == 0 : best\\_probs[i,0] = float('-inf')$\n\nPlease use [math.log](https://docs.python.org/3/library/math.html) to compute the natural logarithm.","metadata":{"id":"44db05c2"}},{"cell_type":"markdown","source":"The example below shows the initialization assuming the corpus starts with the phrase \"Loss tracks upward\".\n\n<img src = \"Initialize4.PNG\"/>","metadata":{"id":"503117ff"}},{"cell_type":"markdown","source":"Represent infinity and negative infinity like this:\n\n```CPP\nfloat('inf')\nfloat('-inf')\n```","metadata":{"id":"79fb223c"}},{"cell_type":"code","source":"# GRADED FUNCTION: initialize\ndef initialize(states, tag_counts, A, B, corpus, vocab):\n    '''\n    Input: \n        states: a list of all possible parts-of-speech\n        tag_counts: a dictionary mapping each tag to its respective count\n        A: Transition Matrix of dimension (num_tags, num_tags)\n        B: Emission Matrix of dimension (num_tags, len(vocab))\n        corpus: a sequence of words whose POS is to be identified in a list \n        vocab: a dictionary where keys are words in vocabulary and value is an index\n    Output:\n        best_probs: matrix of dimension (num_tags, len(corpus)) of floats\n        best_paths: matrix of dimension (num_tags, len(corpus)) of integers\n    '''\n    # Write your code here\n    return best_probs, best_paths","metadata":{"id":"d11b3fe1","execution":{"iopub.status.busy":"2022-03-26T06:56:57.732158Z","iopub.status.idle":"2022-03-26T06:56:57.732655Z","shell.execute_reply.started":"2022-03-26T06:56:57.732440Z","shell.execute_reply":"2022-03-26T06:56:57.732469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)","metadata":{"id":"b859413c","execution":{"iopub.status.busy":"2022-03-26T06:56:57.733652Z","iopub.status.idle":"2022-03-26T06:56:57.733998Z","shell.execute_reply.started":"2022-03-26T06:56:57.733822Z","shell.execute_reply":"2022-03-26T06:56:57.733846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the function\nprint(f\"best_probs[0,0]: {best_probs[0,0]:.4f}\") \nprint(f\"best_paths[2,3]: {best_paths[2,3]:.4f}\")","metadata":{"id":"2a2c438a","execution":{"iopub.status.busy":"2022-03-26T06:56:57.735122Z","iopub.status.idle":"2022-03-26T06:56:57.735453Z","shell.execute_reply.started":"2022-03-26T06:56:57.735274Z","shell.execute_reply":"2022-03-26T06:56:57.735296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Expected Output\n\n```CPP\nbest_probs[0,0]: -22.6098\nbest_paths[2,3]: 0.0000\n```","metadata":{"id":"bdb00300"}},{"cell_type":"markdown","source":"<a name='2.3'></a>\n## Part 2.3 Viterbi Forward\n\nIn this part of the assignment, you will implement the `viterbi_forward` segment. In other words, you will populate your `best_probs` and `best_paths` matrices.\n- Walk forward through the corpus.\n- For each word, compute a probability for each possible tag. \n- Unlike the previous algorithm `predict_pos` (the 'warm-up' exercise), this will include the path up to that (word,tag) combination. \n\nHere is an example with a three-word corpus \"Loss tracks upward\":\n- Note, in this example, only a subset of states (POS tags) are shown in the diagram below, for easier reading. \n- In the diagram below, the first word \"Loss\" is already initialized. \n- The algorithm will compute a probability for each of the potential tags in the second and future words. \n\nCompute the probability that the tag of the second work ('tracks') is a verb, 3rd person singular present (VBZ).  \n- In the `best_probs` matrix, go to the column of the second word ('tracks'), and row 40 (VBZ), this cell is highlighted in light orange in the diagram below.\n- Examine each of the paths from the tags of the first word ('Loss') and choose the most likely path.  \n- An example of the calculation for **one** of those paths is the path from ('Loss', NN) to ('tracks', VBZ).\n- The log of the probability of the path up to and including the first word 'Loss' having POS tag NN is $-14.32$.  The `best_probs` matrix contains this value -14.32 in the column for 'Loss' and row for 'NN'.\n- Find the probability that NN transitions to VBZ.  To find this probability, go to the `A` transition matrix, and go to the row for 'NN' and the column for 'VBZ'.  The value is $4.37e-02$, which is circled in the diagram, so add $-14.32 + log(4.37e-02)$. \n- Find the log of the probability that the tag VBS would 'emit' the word 'tracks'.  To find this, look at the 'B' emission matrix in row 'VBZ' and the column for the word 'tracks'.  The value $4.61e-04$ is circled in the diagram below.  So add $-14.32 + log(4.37e-02) + log(4.61e-04)$.\n- The sum of $-14.32 + log(4.37e-02) + log(4.61e-04)$ is $-25.13$. Store $-25.13$ in the `best_probs` matrix at row 'VBZ' and column 'tracks' (as seen in the cell that is highlighted in light orange in the diagram).\n- All other paths in best_probs are calculated.  Notice that $-25.13$ is greater than all of the other values in column 'tracks' of matrix `best_probs`, and so the most likely path to 'VBZ' is from 'NN'.  'NN' is in row 20 of the `best_probs` matrix, so $20$ is the most likely path.\n- Store the most likely path $20$ in the `best_paths` table.  This is highlighted in light orange in the diagram below.","metadata":{"id":"12457c20"}},{"cell_type":"markdown","source":"The formula to compute the probability and path for the $i^{th}$ word in the $corpus$, the prior word $i-1$ in the corpus, current POS tag $j$, and previous POS tag $k$ is:\n\n$\\mathrm{prob} = \\mathbf{best\\_prob}_{k, i-1} + \\mathrm{log}(\\mathbf{A}_{k, j}) + \\mathrm{log}(\\mathbf{B}_{j, vocab(corpus_{i})})$\n\nwhere $corpus_{i}$ is the word in the corpus at index $i$, and $vocab$ is the dictionary that gets the unique integer that represents a given word.\n\n$\\mathrm{path} = k$\n\nwhere $k$ is the integer representing the previous POS tag.\n","metadata":{"id":"712f47ed"}},{"cell_type":"markdown","source":"<a name='ex-06'></a>\n\n### Exercise 06 - 10 Points\n\nInstructions: Implement the `viterbi_forward` algorithm and store the best_path and best_prob for every possible tag for each word in the matrices `best_probs` and `best_tags` using the pseudo code below.\n\n`for each word in the corpus\n\n    for each POS tag type that this word may be\n    \n        for POS tag type that the previous word could be\n        \n            compute the probability that the previous word had a given POS tag, that the current word has a given POS tag, and that the POS tag would emit this current word.\n            \n            retain the highest probability computed for the current word\n            \n            set best_probs to this highest probability\n            \n            set best_paths to the index 'k', representing the POS tag of the previous word which produced the highest probability `\n\nPlease use [math.log](https://docs.python.org/3/library/math.html) to compute the natural logarithm.","metadata":{"id":"cc160b52"}},{"cell_type":"markdown","source":"<img src = \"Forward4.PNG\"/>","metadata":{"id":"207e8f2c"}},{"cell_type":"code","source":"# GRADED FUNCTION: viterbi_forward\ndef viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab):\n    '''\n    Input: \n        A, B: The transiton and emission matrices respectively\n        test_corpus: a list containing a preprocessed corpus\n        best_probs: an initilized matrix of dimension (num_tags, len(corpus))\n        best_paths: an initilized matrix of dimension (num_tags, len(corpus))\n        vocab: a dictionary where keys are words in vocabulary and value is an index \n    Output: \n        best_probs: a completed matrix of dimension (num_tags, len(corpus))\n        best_paths: a completed matrix of dimension (num_tags, len(corpus))\n    '''\n    # Write your code here\n    return best_probs, best_paths","metadata":{"id":"881830bd","execution":{"iopub.status.busy":"2022-03-26T06:56:57.737159Z","iopub.status.idle":"2022-03-26T06:56:57.737676Z","shell.execute_reply.started":"2022-03-26T06:56:57.737474Z","shell.execute_reply":"2022-03-26T06:56:57.737496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Run the `viterbi_forward` function to fill in the `best_probs` and `best_paths` matrices.\n\n**Note** that this will take a few minutes to run.  There are about 30,000 words to process.","metadata":{"id":"6db1ad34"}},{"cell_type":"code","source":"# this will take a few minutes to run => processes ~ 30,000 words\nbest_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)","metadata":{"id":"63423337","execution":{"iopub.status.busy":"2022-03-26T06:56:57.739168Z","iopub.status.idle":"2022-03-26T06:56:57.739660Z","shell.execute_reply.started":"2022-03-26T06:56:57.739418Z","shell.execute_reply":"2022-03-26T06:56:57.739451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test this function \nprint(f\"best_probs[0,1]: {best_probs[0,1]:.4f}\") \nprint(f\"best_probs[0,4]: {best_probs[0,4]:.4f}\") ","metadata":{"id":"d4af406e","execution":{"iopub.status.busy":"2022-03-26T06:56:57.741043Z","iopub.status.idle":"2022-03-26T06:56:57.741402Z","shell.execute_reply.started":"2022-03-26T06:56:57.741217Z","shell.execute_reply":"2022-03-26T06:56:57.741240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Expected Output\n\n```CPP\nbest_probs[0,1]: -24.7822\nbest_probs[0,4]: -49.5601\n```","metadata":{"id":"81f0b9e2"}},{"cell_type":"markdown","source":"<a name='2.4'></a>\n## Part 2.4 Viterbi backward\n\nNow you will implement the Viterbi backward algorithm.\n- The Viterbi backward algorithm gets the predictions of the POS tags for each word in the corpus using the `best_paths` and the `best_probs` matrices.\n\nThe example below shows how to walk backwards through the best_paths matrix to get the POS tags of each word in the corpus. Recall that this example corpus has three words: \"Loss tracks upward\".\n\nPOS tag for 'upward' is `RB`\n- Select the the most likely POS tag for the last word in the corpus, 'upward' in the `best_prob` table.\n- Look for the row in the column for 'upward' that has the largest probability.\n- Notice that in row 28 of `best_probs`, the estimated probability is -34.99, which is larger than the other values in the column.  So the most likely POS tag for 'upward' is `RB` an adverb, at row 28 of `best_prob`. \n- The variable `z` is an array that stores the unique integer ID of the predicted POS tags for each word in the corpus.  In array z, at position 2, store the value 28 to indicate that the word 'upward' (at index 2 in the corpus), most likely has the POS tag associated with unique ID 28 (which is `RB`).\n- The variable `pred` contains the POS tags in string form.  So `pred` at index 2 stores the string `RB`.\n\n\nPOS tag for 'tracks' is `VBZ`\n- The next step is to go backward one word in the corpus ('tracks').  Since the most likely POS tag for 'upward' is `RB`, which is uniquely identified by integer ID 28, go to the `best_paths` matrix in column 2, row 28.  The value stored in `best_paths`, column 2, row 28 indicates the unique ID of the POS tag of the previous word.  In this case, the value stored here is 40, which is the unique ID for POS tag `VBZ` (verb, 3rd person singular present).\n- So the previous word at index 1 of the corpus ('tracks'), most likely has the POS tag with unique ID 40, which is `VBZ`.\n- In array `z`, store the value 40 at position 1, and for array `pred`, store the string `VBZ` to indicate that the word 'tracks' most likely has POS tag `VBZ`.\n\nPOS tag for 'Loss' is `NN`\n- In `best_paths` at column 1, the unique ID stored at row 40 is 20.  20 is the unique ID for POS tag `NN`.\n- In array `z` at position 0, store 20.  In array `pred` at position 0, store `NN`.","metadata":{"id":"205f3608"}},{"cell_type":"markdown","source":"<img src = \"Backwards5.PNG\"/>","metadata":{"id":"219d0bd1"}},{"cell_type":"markdown","source":"<a name='ex-07'></a>\n### Exercise 07 - 5 Points\nImplement the `viterbi_backward` algorithm, which returns a list of predicted POS tags for each word in the corpus.\n\n- Note that the numbering of the index positions starts at 0 and not 1. \n- `m` is the number of words in the corpus.  \n    - So the indexing into the corpus goes from `0` to `m - 1`.\n    - Also, the columns in `best_probs` and `best_paths` are indexed from `0` to `m - 1`\n\n\n**In Step 1:**       \nLoop through all the rows (POS tags) in the last entry of `best_probs` and find the row (POS tag) with the maximum value.\nConvert the unique integer ID to a tag (a string representation) using the dictionary `states`.  \n\nReferring to the three-word corpus described above:\n- `z[2] = 28`: For the word 'upward' at position 2 in the corpus, the POS tag ID is 28.  Store 28 in `z` at position 2.\n- states(28) is 'RB': The POS tag ID 28 refers to the POS tag 'RB'.\n- `pred[2] = 'RB'`: In array `pred`, store the POS tag for the word 'upward'.\n\n**In Step 2:**  \n- Starting at the last column of best_paths, use `best_probs` to find the most likely POS tag for the last word in the corpus.\n- Then use `best_paths` to find the most likely POS tag for the previous word. \n- Update the POS tag for each word in `z` and in `preds`.\n\nReferring to the three-word example from above, read best_paths at column 2 and fill in z at position 1.  \n`z[1] = best_paths[z[2],2]`  \n\nThe small test following the routine prints the last few words of the corpus and their states to aid in debug.","metadata":{"id":"b9c67c72"}},{"cell_type":"code","source":"# GRADED FUNCTION: viterbi_backward\ndef viterbi_backward(best_probs, best_paths, corpus, states):\n    '''\n    This function returns the best path.\n    \n    '''\n    # Write your code here\n    return pred","metadata":{"id":"8253da58","execution":{"iopub.status.busy":"2022-03-26T06:56:57.742567Z","iopub.status.idle":"2022-03-26T06:56:57.742995Z","shell.execute_reply.started":"2022-03-26T06:56:57.742789Z","shell.execute_reply":"2022-03-26T06:56:57.742828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run and test your function\npred = viterbi_backward(best_probs, best_paths, prep, states)\nm=len(pred)\nprint('The prediction for pred[-7:m-1] is: \\n', prep[-7:m-1], \"\\n\", pred[-7:m-1], \"\\n\")\nprint('The prediction for pred[0:8] is: \\n', pred[0:7], \"\\n\", prep[0:7])","metadata":{"id":"e38c4393","execution":{"iopub.status.busy":"2022-03-26T06:56:57.744229Z","iopub.status.idle":"2022-03-26T06:56:57.744674Z","shell.execute_reply.started":"2022-03-26T06:56:57.744493Z","shell.execute_reply":"2022-03-26T06:56:57.744514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Expected Output:**   \n\n```CPP\nThe prediction for prep[-7:m-1] is:  \n ['see', 'them', 'here', 'with', 'us', '.']  \n ['VB', 'PRP', 'RB', 'IN', 'PRP', '.']   \nThe prediction for pred[0:8] is:    \n ['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN']   \n ['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken'] \n```\n\nNow you just have to compare the predicted labels to the true labels to evaluate your model on the accuracy metric!","metadata":{"id":"46ddeaeb"}},{"cell_type":"markdown","source":"<a name='2.5'></a>\n# Part 2.5: Predicting on a data set\n\nCompute the accuracy of your prediction by comparing it with the true `y` labels. \n- `pred` is a list of predicted POS tags corresponding to the words of the `test_corpus`. ","metadata":{"id":"7b002dd7"}},{"cell_type":"code","source":"print('The third word is:', prep[3])\nprint('Your prediction is:', pred[3])\nprint('Your corresponding label y is: ', y[3])","metadata":{"id":"feec7c33","execution":{"iopub.status.busy":"2022-03-26T06:56:57.745624Z","iopub.status.idle":"2022-03-26T06:56:57.746462Z","shell.execute_reply.started":"2022-03-26T06:56:57.746248Z","shell.execute_reply":"2022-03-26T06:56:57.746277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='ex-08'></a>\n### Exercise 08 - 0 Points\n\nImplement a function to compute the accuracy of the viterbi algorithm's POS tag predictions.\n- To split y into the word and its tag you can use `y.split()`. ","metadata":{"id":"1e8fa19f"}},{"cell_type":"code","source":"# UNGRADED FUNCTION: compute_accuracy (We have implemented this for you)\ndef compute_accuracy(pred, y):\n    '''\n    Input: \n        pred: a list of the predicted parts-of-speech \n        y: a list of lines where each word is separated by a '\\t' (i.e. word \\t tag)\n    Output: \n        \n    '''\n    num_correct = 0\n    total = 0\n    \n    # Zip together the prediction and the labels\n    for prediction, y in zip(pred, y):\n        ### START CODE HERE (Replace instances of 'None' with your code) ###\n        # Split the label into the word and the POS tag\n        word_tag_tuple = y.split()\n        \n        # Check that there is actually a word and a tag\n        # no more and no less than 2 items\n        if len(word_tag_tuple)!=2: # complete this line\n            continue \n\n        # store the word and tag separately\n        word, tag = word_tag_tuple\n        \n        # Check if the POS tag label matches the prediction\n        if prediction == tag: # complete this line\n            \n            # count the number of times that the prediction\n            # and label match\n            num_correct += 1\n            \n        # keep track of the total number of examples (that have valid labels)\n        total += 1\n        \n        ### END CODE HERE ###\n    return num_correct/total","metadata":{"id":"e5092eb3","execution":{"iopub.status.busy":"2022-03-26T06:56:57.748056Z","iopub.status.idle":"2022-03-26T06:56:57.748574Z","shell.execute_reply.started":"2022-03-26T06:56:57.748348Z","shell.execute_reply":"2022-03-26T06:56:57.748375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Accuracy of the Viterbi algorithm is {compute_accuracy(pred, y):.4f}\")","metadata":{"id":"011ad932","execution":{"iopub.status.busy":"2022-03-26T06:56:57.749500Z","iopub.status.idle":"2022-03-26T06:56:57.750335Z","shell.execute_reply.started":"2022-03-26T06:56:57.750123Z","shell.execute_reply":"2022-03-26T06:56:57.750148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Expected Output\n\n```CPP\nAccuracy of the Viterbi algorithm is 0.9531\n```\n\nCongratulations you were able to classify the parts-of-speech with 95% accuracy. ","metadata":{"id":"b74fe8c0"}},{"cell_type":"markdown","source":"<a name='3'></a>\n# Part 3: LSTMs for POS tagging - using PyTorch\nIn this part, We will be building a bidirectional LSTM network to train and inference POS tagging on UDPOS dataset.<br>\n\nPyTorch makes it easy by abstracting most of the details that go in building,training and inferencing a neural network. We recommend going through every PyTorch function that this notebook uses to gain more understanding.   \n\nIf you need a refresher or have never worked with Neural Networks before, here are a few resources:\n- https://web.stanford.edu/~jurafsky/slp3/7.pdf\n- https://web.stanford.edu/~jurafsky/slp3/9.pdf\n- https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n\nWe will be using PyTorch for defining, training and inferencing a neural network for our POS Tagging problem. If you have not used any deep learning framework/library, we recommend you spend some time understanding how to use these libraries. \n\nPyTorch Resources:\n- https://pytorch.org/tutorials/\n- https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html ","metadata":{"id":"b0711aee"}},{"cell_type":"markdown","source":"You will need the following imports. Install these libraries using the following commands. \n- Installing pytorch - https://pytorch.org/get-started/locally/ (choose your setup from here)\n- conda install -c conda-forge spacy\n- conda install -c pytorch torchtext","metadata":{"id":"58b4818c"}},{"cell_type":"markdown","source":"Training a neural network model will take time. \n- Make use of your **Nvidia** GPU if you have one. \n- If not, you can use Google Colab / Kaggle notebooks. You get a free GPU for a limited time to tweak your hyperparameters.\n- Without a GPU, You might have to wait longer to experiment.","metadata":{"id":"06f6b3a6"}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.nn as nn\nimport torch.optim as optim\n\n# a package that provides processing utilities and popular datasets for natural language\nfrom torchtext.legacy import data\nfrom torchtext.legacy.datasets import UDPOS\n\nimport spacy\nfrom tqdm import tqdm \nimport random","metadata":{"id":"afef2ff0","execution":{"iopub.status.busy":"2022-03-26T06:56:57.751439Z","iopub.status.idle":"2022-03-26T06:56:57.751769Z","shell.execute_reply.started":"2022-03-26T06:56:57.751597Z","shell.execute_reply":"2022-03-26T06:56:57.751618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using a seed to maintain consistent and reproducible results\nSEED = 42\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","metadata":{"id":"878101a3","execution":{"iopub.status.busy":"2022-03-26T06:56:57.753176Z","iopub.status.idle":"2022-03-26T06:56:57.753515Z","shell.execute_reply.started":"2022-03-26T06:56:57.753338Z","shell.execute_reply":"2022-03-26T06:56:57.753361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This cell downloads and prepares data, a TorchText Dataset Object\n\nTEXT = data.Field(lower = True)\nUD_TAGS = data.Field(unk_token = None)\nfields = ((\"text\", TEXT), (\"udtags\", UD_TAGS))\ntrain_data, valid_data, test_data = UDPOS.splits(fields)","metadata":{"id":"7f8ba127","execution":{"iopub.status.busy":"2022-03-26T06:56:57.754994Z","iopub.status.idle":"2022-03-26T06:56:57.755588Z","shell.execute_reply.started":"2022-03-26T06:56:57.755402Z","shell.execute_reply":"2022-03-26T06:56:57.755425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing the torchtext dataset","metadata":{"id":"1717eb2b"}},{"cell_type":"code","source":"print(\"Length of the dataset\", len(train_data))\nfor i in range(0,5):\n    print(\"TEXT \", i+1 ,*(train_data[i].__dict__['text']))\n    print(\"TAGS \", i+1 ,*(train_data[i].__dict__['udtags']))","metadata":{"id":"069d103b","execution":{"iopub.status.busy":"2022-03-26T06:56:57.756663Z","iopub.status.idle":"2022-03-26T06:56:57.757340Z","shell.execute_reply.started":"2022-03-26T06:56:57.757055Z","shell.execute_reply":"2022-03-26T06:56:57.757078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### GloVe Vector initialization \nVectorizing the input words is an impotant step in the NLP pipeline that can determine the end performance of neural networks. GloVe vectors capture both global statistics and local statistics of a corpus. We use GloVe to convert words to embeddings in the vector space based on their semantics. \n\nTo learn more about GloVe please read the following resource:\n- https://nlp.stanford.edu/pubs/glove.pdf","metadata":{"id":"66b17bc2"}},{"cell_type":"code","source":"# the words should have atleast a min frequency of 2 to build its vocab\nMIN_FREQ = 2\n\n# Torch text builds the vocabulary based on word representations from glove. \nTEXT.build_vocab(train_data, \n                 min_freq = MIN_FREQ,\n                 vectors = \"glove.6B.100d\",\n                 unk_init = torch.Tensor.normal_)\n\n\nUD_TAGS.build_vocab(train_data)","metadata":{"id":"f1a0c49b","execution":{"iopub.status.busy":"2022-03-26T06:56:57.758608Z","iopub.status.idle":"2022-03-26T06:56:57.759301Z","shell.execute_reply.started":"2022-03-26T06:56:57.759012Z","shell.execute_reply":"2022-03-26T06:56:57.759042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of tags in the dataset\nlen(UD_TAGS.vocab)","metadata":{"id":"b15e4865","execution":{"iopub.status.busy":"2022-03-26T06:56:57.760501Z","iopub.status.idle":"2022-03-26T06:56:57.760973Z","shell.execute_reply.started":"2022-03-26T06:56:57.760705Z","shell.execute_reply":"2022-03-26T06:56:57.760730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Expected output\n18","metadata":{"id":"580b8c2c"}},{"cell_type":"markdown","source":"<a name='3.1'></a>\n# Part 3.1: Building the neural network\n\nWe will make use of the GloVe embeddings and build a bi-directional LSTM. You will be able to tune the hyper parameters of the network and see what works. \n\nIt involves duplicating the first recurrent layer in the network so that there are now two layers side-by-side, then providing the input sequence as-is as input to the first layer and providing a reversed copy of the input sequence to the second.\n\nThe idea is to split the state neurons of a regular RNN in a part that is responsible for the positive time direction (forward states) and a part for the negative time direction (backward states)\n\nMore on it here: https://maxwell.ict.griffith.edu.au/spl/publications/papers/ieeesp97_schuster.pdf\n\nAll the internal computations/details will be taken care by PyTorch. You will be able to implement many variations of this neural networks with minor changes in code. Expect your neural network definition to be under 10 lines.\n\nYour PyTorch model (inherits torch.nn.Module) definition contains defining two functions:\n    -Init : Which specifies what layers to initialize.\n    -Forward: Which defines the order of computations in these layers. <br>\n**Note** - We will not grade based on accuracy, We grade if your model converges. You can follow your order of code, if you think the comments are not helping.  ","metadata":{"id":"73c8c879"}},{"cell_type":"markdown","source":"### Exercise 09 - Building LSTM network - 20 Points","metadata":{"id":"15db110f"}},{"cell_type":"code","source":"class LSTMPOSTagger(nn.Module):\n    def __init__(self, \n                 input_dim, \n                 embedding_dim, \n                 hidden_dim, \n                 output_dim, \n                 n_layers, \n                 bidirectional, \n                 dropout, \n                 pad_idx):\n        \n        super().__init__()\n        \n        # Define an embedding layer that converts the words to embeddings based on GloVe.\n\n        \n        # Define a bi-directional LSTM layer with the hyperparameters. \n\n        \n        # Define a dropout layer that helps in regularization\n\n        \n        # Define a Linear layer which can associate lstm output to the final output \n        \n        \n        \n    def forward(self, text):\n        # pass text through embedding layer\n        \n        # pass embeddings into LSTM\n        \n        # pass the LSTM output to dropout and fully connected linear layer\n        \n        # we use our outputs to make a prediction of what the tag should be\n        \n        # predictions = [sent len, batch size, output dim]\n        \n        return predictions","metadata":{"id":"934c9eb8","execution":{"iopub.status.busy":"2022-03-26T06:56:57.821166Z","iopub.execute_input":"2022-03-26T06:56:57.821435Z","iopub.status.idle":"2022-03-26T06:56:57.842476Z","shell.execute_reply.started":"2022-03-26T06:56:57.821406Z","shell.execute_reply":"2022-03-26T06:56:57.840464Z"},"trusted":true},"execution_count":29,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_34/1313586440.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mLSTMPOSTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     def __init__(self, \n\u001b[1;32m      3\u001b[0m                  \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                  \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"],"ename":"NameError","evalue":"name 'nn' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# Tweak the Nones\nINPUT_DIM = len(TEXT.vocab)\nEMBEDDING_DIM = None\nHIDDEN_DIM = None\nOUTPUT_DIM = len(UD_TAGS.vocab)\nN_LAYERS = None\nBIDIRECTIONAL = None\nDROPOUT = None\nPAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n\nmodel = LSTMPOSTagger(INPUT_DIM, \n                        EMBEDDING_DIM, \n                        HIDDEN_DIM, \n                        OUTPUT_DIM, \n                        N_LAYERS, \n                        BIDIRECTIONAL, \n                        DROPOUT, \n                        PAD_IDX)","metadata":{"id":"e934c279","execution":{"iopub.status.busy":"2022-03-26T06:56:57.844152Z","iopub.status.idle":"2022-03-26T06:56:57.844736Z","shell.execute_reply.started":"2022-03-26T06:56:57.844437Z","shell.execute_reply":"2022-03-26T06:56:57.844475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initializing model weights for better convergence\ndef init_weights(m):\n    for name, param in m.named_parameters():\n        nn.init.normal_(param.data, std=0.1)\nmodel.apply(init_weights)\n\n# initializing model embeddings with glove word vectors\npretrained_embeddings = TEXT.vocab.vectors\nmodel.embedding.weight.data.copy_(pretrained_embeddings)\n\n# making the padding embeddings as all zero, as we don't want to learn paddings.\nmodel.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)","metadata":{"id":"be98840c","execution":{"iopub.status.busy":"2022-03-26T06:56:57.846585Z","iopub.status.idle":"2022-03-26T06:56:57.847355Z","shell.execute_reply.started":"2022-03-26T06:56:57.847047Z","shell.execute_reply":"2022-03-26T06:56:57.847080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If your PC doesn't have enough CPU Ram or Video memory, try decreasing the batch_size\nBATCH_SIZE = 128\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"id":"8d091aa1","execution":{"iopub.status.busy":"2022-03-26T06:56:57.848858Z","iopub.status.idle":"2022-03-26T06:56:57.849552Z","shell.execute_reply.started":"2022-03-26T06:56:57.849251Z","shell.execute_reply":"2022-03-26T06:56:57.849285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BucketIterator allows for data to be split into buckets of equal size,\n# any remaining space is filled with pad token\ntrain_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n    (train_data, valid_data, test_data), \n    batch_size = BATCH_SIZE,\n    device = device)\nTAG_PAD_IDX = UD_TAGS.vocab.stoi[UD_TAGS.pad_token]","metadata":{"id":"da241904","execution":{"iopub.status.busy":"2022-03-26T06:56:57.851029Z","iopub.status.idle":"2022-03-26T06:56:57.851746Z","shell.execute_reply.started":"2022-03-26T06:56:57.851440Z","shell.execute_reply":"2022-03-26T06:56:57.851474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optimizer and Loss Function\nOptimizers are algorithms or methods used to change the attributes of the neural network such as weights and learning rate to reduce the losses. Optimizers are used to solve optimization problems by minimizing the function.\n- PyTorch provides many Optimizer Algorithms, feel free to try them and the one that works best for you. \n- Link - https://pytorch.org/docs/stable/optim.html\n- We will be using CrossEntropyLoss as predicting a word tag is a classification problem.","metadata":{"id":"51d828c6"}},{"cell_type":"code","source":"# optimizer to train the model\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n# ignoring the padding in our loss calculation\ncriterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)","metadata":{"id":"df62b86b","execution":{"iopub.status.busy":"2022-03-26T06:56:57.853456Z","iopub.status.idle":"2022-03-26T06:56:57.854188Z","shell.execute_reply.started":"2022-03-26T06:56:57.853894Z","shell.execute_reply":"2022-03-26T06:56:57.853925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use gpu if available, These lines move your model to gpu from cpu if available\nmodel = model.to(device)\ncriterion = criterion.to(device)\n\n# If this line prints cuda, your machine is equipped with a Nvidia GPU and PyTorch is utilizing the GPU\nprint(device)","metadata":{"id":"ed8f23af","execution":{"iopub.status.busy":"2022-03-26T06:56:57.855559Z","iopub.status.idle":"2022-03-26T06:56:57.856223Z","shell.execute_reply.started":"2022-03-26T06:56:57.855957Z","shell.execute_reply":"2022-03-26T06:56:57.855989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# method to check for accurcy of the model ignoring the pad index\ndef categorical_accuracy(preds, y):\n    \"\"\"\n    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n    \"\"\"\n    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n    non_pad_elements = (y != TAG_PAD_IDX).nonzero()\n    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n    return correct.sum() / y[non_pad_elements].shape[0]","metadata":{"id":"8db84a91","execution":{"iopub.status.busy":"2022-03-26T06:56:57.857380Z","iopub.status.idle":"2022-03-26T06:56:57.858087Z","shell.execute_reply.started":"2022-03-26T06:56:57.857773Z","shell.execute_reply":"2022-03-26T06:56:57.857828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exercise 10 - Training and Testing LSTM - 20 Points\n- Decide your epochs to train based on loss and accuracy\n- Fill single line PyTorch commands. ","metadata":{"id":"85894831"}},{"cell_type":"code","source":"EPOCHS = None\n\nfor epoch in tqdm(range(EPOCHS)):\n    model.train()\n    train_epoch_loss = 0\n    train_epoch_acc = 0\n    print(f'Epoch: {epoch+1:02}\\n')\n    for batch in train_iterator:\n        \n        # returns a batch of text to train on (sent len, batch size)\n        text = batch.text\n        tags = batch.udtags\n        \n        # Add a command that makes the optimizer with zero gradients for each iteration \n         # Add code line\n\n        \n        # Add a command that feeds the batch to the model\n         # Add code line\n\n        \n        # predictions = (sent len, batch size, output dim)\n          # tags = (sent len, batch size)\n        predictions = predictions.view(-1, predictions.shape[-1])\n        tags = tags.view(-1)\n        \n        # Add a command that calculates loss\n         # Add code line\n\n        \n        # Make use of the categorical accuracy and calculate accuracy\n         # Add code line\n\n        \n        # Add a command that calculates gradients\n         # Add code line\n\n        \n        # Add a command that updates the weights by taking steps \n         # Add code line\n            \n        \n        # Calculate loss\n    print(f'\\t [Train Loss] : {train_loss:.3f} | [Train Acc] : {train_acc*100:.2f}%\\n')\n    \n    val_epoch_loss = 0\n    val_epoch_acc = 0\n    \n    \n    \n    # Add a command that moves the model to validation mode\n    model.eval()\n    \n    with torch.no_grad():\n    \n        for batch in valid_iterator:\n\n            text = batch.text\n            tags = batch.udtags\n        \n            # Add the same command that feeds the batch to the model\n            # Add code line\n\n            \n            predictions = predictions.view(-1, predictions.shape[-1])\n            tags = tags.view(-1)\n            \n            # Add the same command that calculates loss\n            # Add code line\n\n            # Make use of the categorical accuracy function and calculate accuracy\n             # Add code line\n            \n            # Calculate validation loss\n        print(f'\\t [Val Loss] : {val_loss:.3f} | [Val Acc] : {val_acc*100:.2f}%\\n')\n\n      ","metadata":{"id":"ef894a29","execution":{"iopub.status.busy":"2022-03-26T06:56:57.859746Z","iopub.status.idle":"2022-03-26T06:56:57.860390Z","shell.execute_reply.started":"2022-03-26T06:56:57.860187Z","shell.execute_reply":"2022-03-26T06:56:57.860210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing the accuracy on test set\ntest_acc=0\nmodel.eval()\n\n# Computes without the gradients. Use this while testing your model.\n# As we do not intend to learn from the data\nwith torch.no_grad():\n    for batch in test_iterator:\n\n    text = batch.text\n    tags = batch.udtags\n\n    # Add the same command that feeds the batch to the model\n    # Add code line\n\n    predictions = predictions.view(-1, predictions.shape[-1])\n    tags = tags.view(-1)\n\n    # Make use of the categorical accuracy function and calculate accuracy\n    # Add code line\n    \n    # Calculate Accuracy\nprint(f'Test Acc: {final_acc*100:.2f}%\\n')","metadata":{"id":"33858a16","execution":{"iopub.status.busy":"2022-03-26T06:56:57.861528Z","iopub.status.idle":"2022-03-26T06:56:57.861863Z","shell.execute_reply.started":"2022-03-26T06:56:57.861671Z","shell.execute_reply":"2022-03-26T06:56:57.861688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Note***: You are using a different dataset compared to part 1 and 2. This part of the assignment is designed/aimed to help develop basic understanding of Neural Networks. Although we expect an accuracy of above 85%, we do not grade based on the accuracy output. ","metadata":{"id":"173e7745"}},{"cell_type":"code","source":"# Try different inputs to these function.\ndef test_lstm(test_sentence):\n    x= test_sentence.unsqueeze(-1).to(device)\n    pred = model(x)\n    pred = pred.argmax(-1)\n    pred_tags = [UD_TAGS.vocab.itos[t.item()] for t in pred]\n    true_tags = [UD_TAGS.vocab.itos[t.item()] for t in test_labels]\n    tokenized_sentence = [TEXT.vocab.itos[t.item()] for t in test_sentence]\n    return tokenized_sentence, true_tags,pred_tags    ","metadata":{"id":"ea018747","execution":{"iopub.status.busy":"2022-03-26T06:56:57.862817Z","iopub.status.idle":"2022-03-26T06:56:57.863227Z","shell.execute_reply.started":"2022-03-26T06:56:57.863049Z","shell.execute_reply":"2022-03-26T06:56:57.863068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sentence = text[0]\ntest_labels = tags[0:len(test_sentence)]\nprint(test_lstm(test_sentence)[0])\nprint(\"True tags\", test_lstm(test_sentence)[1])\nprint(\"Predicted Tags\",test_lstm(test_sentence)[2])","metadata":{"id":"c8c7a491","execution":{"iopub.status.busy":"2022-03-26T06:56:57.864174Z","iopub.status.idle":"2022-03-26T06:56:57.864464Z","shell.execute_reply.started":"2022-03-26T06:56:57.864311Z","shell.execute_reply":"2022-03-26T06:56:57.864326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save your model if it performs well. This saves all the trained weights,\n# so that you don't have to train again in your codewalk\ntorch.save(model.state_dict(), \"./LSTMPOSTAG.pth\")\n\n# loading?\n# model.load_state_dict(torch.load(\"./LSTMPOSTAG.pth\"))","metadata":{"id":"c63cb3d5","execution":{"iopub.status.busy":"2022-03-26T06:56:57.866611Z","iopub.status.idle":"2022-03-26T06:56:57.867105Z","shell.execute_reply.started":"2022-03-26T06:56:57.866764Z","shell.execute_reply":"2022-03-26T06:56:57.866780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Theory Questions: 10 Points\n*** Q1. What is the difference between Count Vectorizer, TFIDF, Word2Vec and GloVe Vectors? ***<br>\n*** Q2. What are the hidden variables in HMM in this assignment? Why are they called hidden? *** <br>\n*** Q3. How Viterbi Algorithm provides more efficient estimation compared to brute force calculation of all tag combinations? *** <br>","metadata":{"id":"43aee2ed"}},{"cell_type":"code","source":"#Question 1","metadata":{"id":"8eca5e9b","execution":{"iopub.status.busy":"2022-03-26T06:57:40.084180Z","iopub.execute_input":"2022-03-26T06:57:40.084842Z","iopub.status.idle":"2022-03-26T06:57:40.088577Z","shell.execute_reply.started":"2022-03-26T06:57:40.084780Z","shell.execute_reply":"2022-03-26T06:57:40.087675Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"#Question 1","metadata":{}},{"cell_type":"markdown","source":" Q1. What is the difference between Count Vectorizer, TFIDF, Word2Vec and GloVe Vectors? *\n \n One of the major challenges that any NLP Data Scientist faces is to choose the best possible numerical/vectorial representation of the text strings for running Machine Learning models. In a general scenario, we work on a bunch of text information that may or may not be tagged and we then want to build a ML model that can understand the pattern based on the words present in the strings to predict a new text data.\n\nFor example, we know that \"Its raining heavily outside\" and \"Its pouring down outside\" mean the same thing. But how to make the computer understand it. As far as we know, computers can only understand numerical data, while natural language data, for computers, are just text strings without any numerical or statistical information.\n\nSo how to bridge the gap? How to make computers understand text data?\n\nBy converting texts to numbers and also conserving linguistic information for analysis.\n\nIn this article i am going to discuss about 2 different ways of converting Text to Numbers for analysis.\n\nCount Vectorizers:\nCount Vectorizer is a way to convert a given set of strings into a frequency representation.\n\nLets take this example:\n\nText1 = Natural Language Processing is a subfield of AI\ntag1 = \"NLP\"\n\nText2 = Computer Vision is a subfield of AI\ntag2 = \"CV\"\n\nIn the above two examples you have Texts that are Tagged respectively. This is a very simple case of NLP where you get tagged text data set and then using it you have to predict the tag of another text data.\n\nThe above two texts can be converted into count frequency using the CountVectorizer function of sklearn library:\n\nfrom sklearn.feature_extraction.text import CountVectorizer as CV\nimport pandas as pd\ncv = CV()\ncv.fit([Text1, Text2])\nx= cv.transform([Text1]).toarray()\ny= cv.transform([Text2]).toarray()\ncolumns = cv.get_feature_names()\ndf1 = pd.DataFrame(x, columns= columns, index= [\"Text1\"])\ndf2 = pd.DataFrame(y, columns= columns, index= [\"Text2\"])\ndf = pd.concat([df1,df2])\ndf[\"tag\"] = [\"NLP\", \"CV\"]\ndf\n\nNo alt text provided for this image\nSince Text1 doesn't contain words \"computer\" and \"vision\", hence their frequencies are calculated as 0 while other words are present once hence their frequencies are equal to 1.\n\nThis is, in a nutshell, how we use Count Vectorizer!\n\nCount Vectors can be helpful in understanding the type of text by the frequency of words in it. But its major disadvantages are:\n\nIts inability in identifying more important and less important words for analysis.\nIt will just consider words that are abundant in a corpus as the most statistically significant word.\nIt also doesn't identify the relationships between words such as linguistic similarity between words.\nTF-IDF:\nTF-IDF means Term Frequency - Inverse Document Frequency. This is a statistic that is based on the frequency of a word in the corpus but it also provides a numerical representation of how important a word is for statistical analysis.\n\nTF-IDF is better than Count Vectorizers because it not only focuses on the frequency of words present in the corpus but also provides the importance of the words. We can then remove the words that are less important for analysis, hence making the model building less complex by reducing the input dimensions.\n\nThis is how tf-idf is calculated:\n\nNo alt text provided for this image\nThe term \"tf\" is basically the count of a word in a sentence. for example, in the above two examples for Text1, the tf value of the word \"subfield\" will be 1.\n\nthe term \"df\" is called document frequency which means in how many documents the word \"subfield\" is present within corpus. In our case the corpus consists of Text1 and Text2 (N value is 2) and the word \"subfield\" is present in both. Hence its df value is 2.\n\nAlso since in the formula df is present in the denominator of log (N/df) its called inverse document frequency. Hence the name tf-idf.\n\nHere is how we calculate tfidf for a corpus:\n\nText1 = Natural Language Processing is a subfield of AI\ntag1 = \"NLP\"\n\nText2 = Computer Vision is a subfield of AI\ntag2 = \"CV\"\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer as tf_idf\nimport pandas as pd\ntfidf = tf_idf(norm = None)\ntfidf.fit([Text1, Text2])\nx= tfidf.transform([Text1]).toarray()\ny= tfidf.transform([Text2]).toarray()\ncolumns = tfidf.get_feature_names()\ndf1 = pd.DataFrame(x, columns= columns, index= [\"Text1\"])\ndf2 = pd.DataFrame(y, columns= columns, index= [\"Text2\"])\ndf = pd.concat([df1,df2])\ndf[\"tag\"] = [\"NLP\", \"CV\"]\ndf\n\nNo alt text provided for this image\nHere sklearn calculated the idf value as:\n\nidf = ln[(1+N)/(1+df)]+1\nFor Text1 the calculation is:\n\nIn our example since there are only 2 texts/documents in the corpus, hence N=2\n\nNo alt text provided for this image\nImportance of Words:\n\nTFIDF is based on the logic that words that are too abundant in a corpus and words that are too rare are both not statistically important for finding a pattern. The Logarithmic factor in tfidf mathematically penalizes the words that are too abundant or too rare in the corpus by giving them low tfidf scores.\n\nHigher value of tfidf signifies higher importance of the words in the corpus while lower values represent lower importance. In the above example the word \"AI\" is present in both the sentences while words \"Natural\" and \"Computer\" are present only in one sentences each. Hence the tfidf value of \"AI\" is lower than the other two. While for the word \"Natural\" there are more words in Text1 hence its importance is lower than \"Computer\" since there are less number of words in Text2.\n\nEven though TFIDF can provide a good understanding about the importance of words but just like Count Vectors, its disadvantage is:\n\nIt fails to provide linguistic information about the words such as the real meaning of the words, similarity with other words etc.\nTo train a model on the actual linguistic relationship of the words, there are two other word embedding techniques widely used in NLP, they are \"word2vec\" and \"Glove\". I will discuss about these two in another articl\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":" Q2. What are the hidden variables in HMM in this assignment? Why are they called hidden? *\n \n \n ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":" Q3. How Viterbi Algorithm provides more efficient estimation compared to brute force calculation of all tag combinations? *\n \n ","metadata":{}}]}